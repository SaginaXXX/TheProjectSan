# â“ S3é›†æˆå¸¸è§é—®é¢˜ FAQ

## ğŸ¯ é…ç½®ç›¸å…³

### Q1: å¿…é¡»ä½¿ç”¨AWS S3å—ï¼Ÿå¯ä»¥ç”¨é˜¿é‡Œäº‘OSSå—ï¼Ÿ

**A**: å¯ä»¥ï¼S3StorageServiceä½¿ç”¨boto3åº“ï¼Œå®ƒå…¼å®¹S3åè®®çš„å¯¹è±¡å­˜å‚¨æœåŠ¡ã€‚

**é˜¿é‡Œäº‘OSSé…ç½®**:
```python
# src/ai_chat/storage/s3_service.py
# ä¿®æ”¹S3å®¢æˆ·ç«¯åˆå§‹åŒ–

self.s3_client = boto3.client(
    's3',
    region_name=region,
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    endpoint_url='https://oss-cn-hangzhou.aliyuncs.com'  # â† æ·»åŠ endpoint
)
```

**æ”¯æŒçš„æœåŠ¡å•†**:
- âœ… AWS S3
- âœ… é˜¿é‡Œäº‘OSSï¼ˆéœ€é…ç½®endpointï¼‰
- âœ… è…¾è®¯äº‘COSï¼ˆéœ€é…ç½®endpointï¼‰
- âœ… MinIOï¼ˆè‡ªå»ºï¼Œéœ€é…ç½®endpointï¼‰
- âœ… Backblaze B2ï¼ˆéœ€é…ç½®endpointï¼‰
- âœ… DigitalOcean Spaces

---

### Q2: storage_typeé…ç½®åœ¨å“ªé‡Œï¼Ÿå¦‚ä½•åˆ‡æ¢ï¼Ÿ

**A**: åœ¨`conf.yaml`æ–‡ä»¶ä¸­é…ç½®ï¼š

```yaml
system_config:
  media_server:
    storage_type: "local"  # æˆ– "s3"
```

**åˆ‡æ¢æ­¥éª¤**:
```bash
# 1. ä¿®æ”¹é…ç½®æ–‡ä»¶
vim conf.yaml
# å°† storage_type: "local" æ”¹ä¸º storage_type: "s3"

# 2. é…ç½®S3å‚æ•°ï¼ˆé¦–æ¬¡ï¼‰
# æ·»åŠ  s3_bucket, s3_region ç­‰

# 3. é‡å¯æœåŠ¡å™¨
python run_server.py

# âœ… è‡ªåŠ¨åˆ‡æ¢åˆ°S3æ¨¡å¼
```

**æ— éœ€ä¿®æ”¹ä»£ç **ï¼Œåªéœ€ä¿®æ”¹é…ç½®ï¼

---

### Q3: S3å‡­è¯åº”è¯¥å†™åœ¨é…ç½®æ–‡ä»¶è¿˜æ˜¯ç¯å¢ƒå˜é‡ï¼Ÿ

**A**: å¼ºçƒˆæ¨èä½¿ç”¨**ç¯å¢ƒå˜é‡**ï¼Œé¿å…æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚

**âŒ ä¸æ¨èï¼ˆå®‰å…¨é£é™©ï¼‰**:
```yaml
# conf.yaml
s3_access_key: "AKIAIOSFODNN7EXAMPLE"  # âŒ ä¼šè¢«æäº¤åˆ°Git
s3_secret_key: "wJalrXUtnFEMI/..."     # âŒ æ•æ„Ÿä¿¡æ¯æ³„éœ²
```

**âœ… æ¨è**:
```yaml
# conf.yaml
s3_bucket: "my-bucket"  # âœ… å¯ä»¥å…¬å¼€
s3_region: "us-east-1"  # âœ… å¯ä»¥å…¬å¼€
# s3_access_key ç•™ç©º
# s3_secret_key ç•™ç©º
```

```bash
# .env (ä¸æäº¤åˆ°Git)
AWS_ACCESS_KEY=AKIAIOSFODNN7EXAMPLE
AWS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
```

**.gitignore**:
```
.env
.env.local
.env.production
*.secret
```

---

### Q4: CDNæ˜¯å¿…é¡»çš„å—ï¼Ÿ

**A**: ä¸å¿…é¡»ï¼Œä½†**å¼ºçƒˆæ¨è**ã€‚

**ä¸ä½¿ç”¨CDN**:
```python
# S3ç›´è¿URLï¼ˆæ…¢ï¼‰
https://my-bucket.s3.us-east-1.amazonaws.com/client_001/ads/video.mp4

ç¼ºç‚¹:
âŒ é€Ÿåº¦æ…¢ï¼ˆè·¨åŒºåŸŸå»¶è¿Ÿé«˜ï¼‰
âŒ S3è¯·æ±‚æˆæœ¬é«˜
âŒ æ— æ³•åˆ©ç”¨è¾¹ç¼˜ç¼“å­˜
```

**ä½¿ç”¨CDN**:
```python
# CDNåŠ é€ŸURLï¼ˆå¿«ï¼‰
https://cdn.example.com/client_001/ads/video.mp4

ä¼˜ç‚¹:
âœ… å…¨çƒåŠ é€Ÿï¼ˆè¾¹ç¼˜èŠ‚ç‚¹ï¼‰
âœ… é™ä½S3æˆæœ¬
âœ… è‡ªåŠ¨HTTPS
âœ… ç¼“å­˜ä¼˜åŒ–
âœ… å¸¦å®½ä¼˜åŒ–
```

**æ¨è**: ç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨CDN

---

## ğŸ” å®‰å…¨ç›¸å…³

### Q5: å¦‚ä½•ç¡®ä¿ä¸åŒCLIENT_IDçš„æ•°æ®éš”ç¦»ï¼Ÿ

**A**: é¡¹ç›®é‡‡ç”¨**å¤šå±‚éš”ç¦»æœºåˆ¶**ï¼š

**1. S3 Keyå‰ç¼€éš”ç¦»**:
```
client_001/ads/video1.mp4  â† æ˜Ÿå·´å…‹
client_002/ads/video2.mp4  â† éº¦å½“åŠ³

âœ… ä¸åŒå‰ç¼€ï¼Œé€»è¾‘éš”ç¦»
```

**2. APIå‚æ•°éªŒè¯**:
```python
# ä¸Šä¼ æ—¶éªŒè¯CLIENT_ID
client_id = request.form.get('client')
if client_id != expected_client_id:
    raise PermissionError("CLIENT_IDä¸åŒ¹é…")
```

**3. IAMç­–ç•¥éš”ç¦»**ï¼ˆå¯é€‰ï¼Œæœ€å®‰å…¨ï¼‰:
```json
{
    "Effect": "Allow",
    "Action": ["s3:PutObject"],
    "Resource": ["arn:aws:s3:::bucket/client_001/*"],
    "Condition": {
        "StringLike": {"s3:prefix": ["client_001/*"]}
    }
}
```

**4. åº”ç”¨å±‚è¿‡æ»¤**:
```python
# åˆ—è¡¨æ—¶åªè¿”å›å½“å‰CLIENT_IDçš„æ–‡ä»¶
async def list_files(category: str):
    prefix = f"{self.client_id}/{category}/"  # åªæ‰«æè‡ªå·±çš„å‰ç¼€
    return files
```

---

### Q6: S3æ¡¶åº”è¯¥è®¾ç½®ä¸ºå…¬å¼€è¿˜æ˜¯ç§æœ‰ï¼Ÿ

**A**: å–å†³äºæ˜¯å¦ä½¿ç”¨CDNã€‚

**æ–¹æ¡ˆA: å…¬å¼€è¯»å–ï¼ˆæ¨èï¼Œç®€å•ï¼‰**
```bash
# S3æ¡¶ç­–ç•¥
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::my-bucket/*"
        }
    ]
}

ä¼˜ç‚¹:
âœ… CDNç›´æ¥è®¿é—®ï¼Œæ— éœ€ç­¾å
âœ… é…ç½®ç®€å•
âœ… æ€§èƒ½å¥½

ç¼ºç‚¹:
âŒ ä»»ä½•äººéƒ½èƒ½è®¿é—®ï¼ˆå¦‚æœçŸ¥é“URLï¼‰

é€‚ç”¨: å¹¿å‘Šè§†é¢‘ç­‰å…¬å¼€å†…å®¹
```

**æ–¹æ¡ˆB: ç§æœ‰æ¡¶ + é¢„ç­¾åURLï¼ˆå®‰å…¨ï¼‰**
```python
# ç”Ÿæˆæœ‰æ—¶æ•ˆçš„é¢„ç­¾åURL
def get_file_url(self, category: str, filename: str) -> str:
    s3_key = self._get_s3_key(category, filename)
    
    # ç”Ÿæˆ1å°æ—¶æœ‰æ•ˆçš„ç­¾åURL
    url = self.s3_client.generate_presigned_url(
        'get_object',
        Params={'Bucket': self.bucket, 'Key': s3_key},
        ExpiresIn=3600  # 1å°æ—¶
    )
    return url

ä¼˜ç‚¹:
âœ… å®Œå…¨ç§æœ‰ï¼Œå®‰å…¨
âœ… URLæœ‰æ—¶æ•ˆæ€§
âœ… å¯æ’¤é”€è®¿é—®

ç¼ºç‚¹:
âŒ é…ç½®å¤æ‚
âŒ CDNéœ€è¦ç‰¹æ®Šé…ç½®
âŒ æ€§èƒ½ç¨å·®ï¼ˆæ¯æ¬¡ç”ŸæˆURLï¼‰

é€‚ç”¨: æ•æ„Ÿå†…å®¹ã€ä»˜è´¹å†…å®¹
```

**æ¨è**: å¹¿å‘Šè§†é¢‘ä½¿ç”¨**å…¬å¼€æ¡¶**ï¼Œç®€å•é«˜æ•ˆ

---

## ğŸ“¦ éƒ¨ç½²ç›¸å…³

### Q7: å¯ä»¥åœ¨æœ¬åœ°å¼€å‘æ—¶ä½¿ç”¨S3å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†**ä¸æ¨è**ã€‚

**æœ¬åœ°å¼€å‘**:
```yaml
# conf.yaml (å¼€å‘ç¯å¢ƒ)
storage_type: "local"  # â† ä½¿ç”¨æœ¬åœ°å­˜å‚¨
```

**ç”Ÿäº§éƒ¨ç½²**:
```yaml
# conf.yaml (ç”Ÿäº§ç¯å¢ƒ)
storage_type: "s3"     # â† ä½¿ç”¨S3å­˜å‚¨
```

**åŸå› **:
- âœ… æœ¬åœ°å¼€å‘å¿«é€Ÿè¿­ä»£ï¼Œæ— éœ€ç½‘ç»œ
- âœ… èŠ‚çœS3æˆæœ¬
- âœ… ç¦»çº¿å¼€å‘

**æµ‹è¯•S3**:
```bash
# ä¸´æ—¶åˆ‡æ¢åˆ°S3æµ‹è¯•
export STORAGE_TYPE=s3
export S3_BUCKET=test-bucket
python run_server.py

# æµ‹è¯•å®Œæ¯•åˆ‡å›æœ¬åœ°
unset STORAGE_TYPE
```

---

### Q8: å¤šä¸ªå®¹å™¨å¯ä»¥å…±äº«ä¸€ä¸ªS3æ¡¶å—ï¼Ÿ

**A**: å¯ä»¥ï¼è¿™æ˜¯æ¨èçš„éƒ¨ç½²æ–¹å¼ã€‚

```
3ä¸ªå®¹å™¨ + 1ä¸ªS3æ¡¶ = âœ… å®Œç¾

Container 1 (CLIENT_ID=client_001)  â”€â”
Container 2 (CLIENT_ID=client_002)  â”€â”¼â†’ s3://my-bucket/
Container 3 (CLIENT_ID=client_003)  â”€â”˜    â”œâ”€ client_001/
                                           â”œâ”€ client_002/
                                           â””â”€ client_003/

âœ… ç»Ÿä¸€ç®¡ç†
âœ… é™ä½æˆæœ¬ï¼ˆåªéœ€1ä¸ªæ¡¶ï¼‰
âœ… CLIENT_IDå‰ç¼€éš”ç¦»ï¼Œäº’ä¸å¹²æ‰°
```

**éš”ç¦»ä¿è¯**:
- Container 1ä¸Šä¼  â†’ `client_001/ads/`
- Container 2ä¸Šä¼  â†’ `client_002/ads/`
- Container 3ä¸Šä¼  â†’ `client_003/ads/`
- âœ… æ°¸ä¸å†²çª

---

### Q9: Dockeréƒ¨ç½²æ—¶CLIENT_IDå¦‚ä½•é…ç½®ï¼Ÿ

**A**: é€šè¿‡**ç¯å¢ƒå˜é‡**é…ç½®ï¼Œæ¯ä¸ªå®¹å™¨ä¸åŒã€‚

```yaml
# docker-compose.yml
services:
  backend_client001:
    environment:
      - CLIENT_ID=client_001  # â† å®¹å™¨1çš„CLIENT_ID
  
  backend_client002:
    environment:
      - CLIENT_ID=client_002  # â† å®¹å™¨2çš„CLIENT_ID
```

**ä¼˜å…ˆçº§**:
```
ç¯å¢ƒå˜é‡ CLIENT_ID  (æœ€é«˜ï¼ŒDockerä½¿ç”¨)
    â†“
é…ç½®æ–‡ä»¶ conf.yaml  (æ¬¡çº§ï¼Œæœ¬åœ°å¼€å‘ä½¿ç”¨)
    â†“
é»˜è®¤å€¼ default_client (å…œåº•)
```

---

## ğŸš€ æ€§èƒ½ç›¸å…³

### Q10: S3ä¸Šä¼ é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: å¤šç§ä¼˜åŒ–æ–¹æ¡ˆã€‚

**æ–¹æ¡ˆ1: ä½¿ç”¨S3ä¼ è¾“åŠ é€Ÿ**
```python
# å¯ç”¨ä¼ è¾“åŠ é€Ÿ
self.s3_client = boto3.client(
    's3',
    config=Config(
        s3={'use_accelerate_endpoint': True}  # â† ä¼ è¾“åŠ é€Ÿ
    )
)

# éœ€è¦åœ¨S3æ¡¶å¯ç”¨ä¼ è¾“åŠ é€Ÿ
aws s3api put-bucket-accelerate-configuration \
    --bucket my-bucket \
    --accelerate-configuration Status=Enabled

ä¼˜åŠ¿:
âœ… å…¨çƒä¸Šä¼ åŠ é€Ÿ50-500%
âœ… è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è·¯å¾„

æˆæœ¬:
ğŸ’° æ¯GBé¢å¤– $0.04
```

**æ–¹æ¡ˆ2: é€‰æ‹©å°±è¿‘åŒºåŸŸ**
```yaml
# ä¸­å›½ç”¨æˆ·ä½¿ç”¨ä¸­å›½åŒºåŸŸ
s3_region: "cn-north-1"  # AWSä¸­å›½ï¼ˆåŒ—äº¬ï¼‰

# æˆ–ä½¿ç”¨é˜¿é‡Œäº‘OSS
storage_type: "s3"
endpoint_url: "https://oss-cn-hangzhou.aliyuncs.com"
```

**æ–¹æ¡ˆ3: å¤šéƒ¨åˆ†ä¸Šä¼ ï¼ˆå¤§æ–‡ä»¶ï¼‰**
```python
# å¤§äº5MBçš„æ–‡ä»¶ä½¿ç”¨å¤šéƒ¨åˆ†ä¸Šä¼ 
if file_size > 5 * 1024 * 1024:
    # åˆ†ç‰‡ä¸Šä¼ ï¼Œæé«˜æˆåŠŸç‡
    s3_client.upload_fileobj(
        file_data,
        bucket,
        key,
        Config=TransferConfig(
            multipart_threshold=5 * 1024 * 1024,
            multipart_chunksize=5 * 1024 * 1024
        )
    )
```

---

### Q11: CDNç¼“å­˜è¿‡æœŸæ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨**ç¼“å­˜å¤±æ•ˆAPI**æˆ–**ç‰ˆæœ¬å·ç­–ç•¥**ã€‚

**æ–¹æ¡ˆ1: CloudFrontç¼“å­˜å¤±æ•ˆ**
```python
# ä¸Šä¼ æ–°æ–‡ä»¶åï¼Œè®©CDNå¤±æ•ˆæ—§ç¼“å­˜
import boto3

cloudfront = boto3.client('cloudfront')
cloudfront.create_invalidation(
    DistributionId='E1234567890ABC',
    InvalidationBatch={
        'Paths': {
            'Quantity': 1,
            'Items': ['/client_001/ads/*']
        },
        'CallerReference': str(int(time.time()))
    }
)

ä¼˜ç‚¹:
âœ… ç«‹å³ç”Ÿæ•ˆ
âœ… çµæ´»æ§åˆ¶

ç¼ºç‚¹:
âŒ æœ‰æˆæœ¬ï¼ˆå‰1000æ¬¡/æœˆå…è´¹ï¼‰
âŒ éœ€è¦é¢å¤–APIè°ƒç”¨
```

**æ–¹æ¡ˆ2: ç‰ˆæœ¬å·ç­–ç•¥ï¼ˆæ¨èï¼‰**
```python
# æ–‡ä»¶ååŒ…å«æ—¶é—´æˆ³ï¼Œè‡ªåŠ¨ç‰ˆæœ¬åŒ–
filename = f"video_{timestamp}.mp4"

# URLè‡ªåŠ¨å˜åŒ–
# æ—§: https://cdn.example.com/client_001/ads/video_1730000000.mp4
# æ–°: https://cdn.example.com/client_001/ads/video_1730000100.mp4

ä¼˜ç‚¹:
âœ… é›¶æˆæœ¬
âœ… æ— éœ€å¤±æ•ˆ
âœ… æ—§æ–‡ä»¶å¯ä¿ç•™

ç¼ºç‚¹:
âŒ æ–‡ä»¶åå†—é•¿
```

**é¡¹ç›®å·²é‡‡ç”¨**: æ–¹æ¡ˆ2ï¼ˆæ—¶é—´æˆ³ç‰ˆæœ¬åŒ–ï¼‰

---

### Q12: æœ¬åœ°ç¼“å­˜å¦‚ä½•é…ç½®ï¼Ÿ

**A**: S3æ¨¡å¼ä¸‹ä»å¯ä½¿ç”¨æœ¬åœ°ç¼“å­˜æé€Ÿã€‚

```yaml
# conf.yaml
system_config:
  media_server:
    storage_type: "s3"
    
    # ç¼“å­˜é…ç½®
    cache_enabled: true         # â† å¯ç”¨ç¼“å­˜
    cache_dir: "cache"          # â† ç¼“å­˜ç›®å½•
    cache_size_limit: "10GB"    # â† ç¼“å­˜å¤§å°é™åˆ¶
```

**ç¼“å­˜ç­–ç•¥**:
```python
# æ™ºèƒ½ç¼“å­˜ï¼ˆæœªæ¥å®ç°ï¼‰
async def get_file_with_cache(category: str, filename: str):
    cache_path = f"cache/{client_id}/{category}/{filename}"
    
    # 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    if os.path.exists(cache_path):
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆä¾‹å¦‚24å°æ—¶ï¼‰
        if time.time() - os.path.getmtime(cache_path) < 86400:
            return cache_path  # ä½¿ç”¨ç¼“å­˜
    
    # 2. ä»S3ä¸‹è½½
    file_data = await s3_client.get_object(...)
    
    # 3. ä¿å­˜åˆ°ç¼“å­˜
    with open(cache_path, 'wb') as f:
        f.write(file_data)
    
    return cache_path
```

**ä¼˜åŠ¿**:
- âœ… ç¬¬ä¸€æ¬¡ä»S3åŠ è½½
- âœ… åç»­ä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼ˆå¿«ï¼‰
- âœ… è‡ªåŠ¨æ¸…ç†æ—§ç¼“å­˜
- âœ… èŠ‚çœS3è¯·æ±‚æˆæœ¬

---

## ğŸ”§ æ•…éšœæ’é™¤

### Q13: ä¸Šä¼ åˆ°S3å¤±è´¥ï¼Œæç¤º"Access Denied"

**åŸå› **: IAMæƒé™ä¸è¶³

**è§£å†³**:
```bash
# 1. æ£€æŸ¥IAMç”¨æˆ·æƒé™
aws iam get-user-policy --user-name ai-screen-uploader --policy-name S3FullAccess

# 2. ç¡®ä¿æœ‰ä»¥ä¸‹æƒé™
{
    "Action": [
        "s3:PutObject",      # ä¸Šä¼ 
        "s3:GetObject",      # è¯»å–
        "s3:DeleteObject",   # åˆ é™¤
        "s3:ListBucket"      # åˆ—è¡¨
    ]
}

# 3. æ£€æŸ¥æ¡¶ç­–ç•¥
aws s3api get-bucket-policy --bucket my-bucket

# 4. æµ‹è¯•å‡­è¯
aws s3 ls s3://my-bucket --profile ai-screen
```

---

### Q14: CDNè®¿é—®è¿”å›403 Forbidden

**åŸå› **: CORSé…ç½®é—®é¢˜æˆ–æ¡¶æƒé™é—®é¢˜

**è§£å†³**:
```bash
# 1. æ£€æŸ¥S3 CORSé…ç½®
aws s3api get-bucket-cors --bucket my-bucket

# 2. æ·»åŠ CORSè§„åˆ™
{
    "CORSRules": [
        {
            "AllowedOrigins": ["*"],
            "AllowedMethods": ["GET", "HEAD"],
            "AllowedHeaders": ["*"],
            "MaxAgeSeconds": 3600
        }
    ]
}

# 3. æ£€æŸ¥CDN Originé…ç½®
# ç¡®ä¿CDNèƒ½è®¿é—®S3æºç«™

# 4. æµ‹è¯•ç›´æ¥è®¿é—®S3
curl -I https://my-bucket.s3.us-east-1.amazonaws.com/client_001/ads/test.mp4
# åº”è¯¥è¿”å› 200 OK
```

---

### Q15: MCPæœåŠ¡å™¨æ‰¾ä¸åˆ°å¹¿å‘Šè§†é¢‘

**åŸå› **: MCPæœåŠ¡å™¨æœªä½¿ç”¨storage_service

**è§£å†³**:
```python
# âŒ é—®é¢˜ä»£ç 
self.ads_dir = Path("ads") / self.client_id
for file in self.ads_dir.iterdir():  # S3æ¨¡å¼ä¸‹ç›®å½•ä¸å­˜åœ¨

# âœ… ä¿®æ”¹ä¸º
self.storage_service = create_storage_service(config, client_id)
files = await self.storage_service.list_files("ads")  # è‡ªåŠ¨é€‚é…S3
```

**éªŒè¯**:
```bash
# 1. æŸ¥çœ‹MCPæœåŠ¡å™¨æ—¥å¿—
# åº”è¯¥æ˜¾ç¤º: "ä½¿ç”¨å­˜å‚¨æœåŠ¡: S3StorageService"

# 2. æ£€æŸ¥S3ä¸­çš„æ–‡ä»¶
aws s3 ls s3://my-bucket/client_001/ads/

# 3. è°ƒç”¨MCP refreshå·¥å…·
# åº”è¯¥èƒ½åˆ·æ–°S3ä¸­çš„æ–‡ä»¶
```

---

## ğŸ’° æˆæœ¬ç›¸å…³

### Q16: S3æˆæœ¬å¦‚ä½•æ§åˆ¶ï¼Ÿ

**A**: å¤šç§ä¼˜åŒ–ç­–ç•¥ã€‚

**1. ä½¿ç”¨æ™ºèƒ½åˆ†å±‚å­˜å‚¨**
```python
# ä¸Šä¼ æ—¶è®¾ç½®å­˜å‚¨ç±»
s3_client.put_object(
    Bucket=bucket,
    Key=key,
    Body=data,
    StorageClass='INTELLIGENT_TIERING'  # â† æ™ºèƒ½åˆ†å±‚
)

ä¼˜åŠ¿:
âœ… è‡ªåŠ¨ä¼˜åŒ–å­˜å‚¨ç±»
âœ… 30å¤©æœªè®¿é—® â†’ å½’æ¡£å±‚ï¼ˆçœ80%ï¼‰
âœ… è®¿é—®æ—¶è‡ªåŠ¨æ¢å¤
```

**2. è®¾ç½®ç”Ÿå‘½å‘¨æœŸè§„åˆ™**
```json
{
    "Rules": [
        {
            "Id": "DeleteOldAds",
            "Status": "Enabled",
            "Filter": {
                "Prefix": "client_001/ads/"
            },
            "Expiration": {
                "Days": 365  # 1å¹´åè‡ªåŠ¨åˆ é™¤
            }
        }
    ]
}

ä¼˜åŠ¿:
âœ… è‡ªåŠ¨æ¸…ç†æ—§æ–‡ä»¶
âœ… é™ä½å­˜å‚¨æˆæœ¬
```

**3. ä½¿ç”¨Cloudflare CDN**
```
S3ä¸‹è½½æµé‡æˆæœ¬: $0.09/GB
Cloudflare CDNæµé‡: å…è´¹ï¼ˆæ— é™ï¼‰

âœ… èŠ‚çœ90%æµé‡æˆæœ¬ï¼
```

**4. é€‰æ‹©ä½æˆæœ¬æœåŠ¡å•†**
```
Backblaze B2:
- å­˜å‚¨: $0.005/GB/æœˆ (AWSçš„1/5)
- ä¸‹è½½: å‰10GBå…è´¹
- æ€»æˆæœ¬: ~$5/æœˆï¼ˆ100GBï¼‰

AWS S3:
- å­˜å‚¨: $0.023/GB/æœˆ
- ä¸‹è½½: $0.09/GB
- æ€»æˆæœ¬: ~$20/æœˆï¼ˆ100GBï¼‰

âœ… B2çœ75%æˆæœ¬ï¼
```

---

### Q17: å¦‚ä½•ä¼°ç®—æˆ‘çš„S3æˆæœ¬ï¼Ÿ

**A**: ä½¿ç”¨ä»¥ä¸‹å…¬å¼ã€‚

**æˆæœ¬è®¡ç®—å™¨**:
```python
# è¾“å…¥å‚æ•°
storage_gb = 100            # å­˜å‚¨é‡ï¼ˆGBï¼‰
downloads_gb_per_month = 200  # æœˆä¸‹è½½é‡ï¼ˆGBï¼‰
requests_per_month = 500000   # æœˆè¯·æ±‚æ•°

# AWS S3æˆæœ¬
storage_cost = storage_gb * 0.023                    # å­˜å‚¨è´¹ç”¨
download_cost = downloads_gb_per_month * 0.09        # æµé‡è´¹ç”¨
request_cost = (requests_per_month / 1000) * 0.0004  # è¯·æ±‚è´¹ç”¨

total_cost = storage_cost + download_cost + request_cost

print(f"æœˆåº¦æ€»æˆæœ¬: ${total_cost:.2f}")
# è¾“å‡º: æœˆåº¦æ€»æˆæœ¬: $20.50

# Backblaze B2æˆæœ¬
storage_cost_b2 = storage_gb * 0.005
download_cost_b2 = max(0, (downloads_gb_per_month - 10)) * 0.01  # å‰10GBå…è´¹

total_cost_b2 = storage_cost_b2 + download_cost_b2

print(f"B2æœˆåº¦æ€»æˆæœ¬: ${total_cost_b2:.2f}")
# è¾“å‡º: B2æœˆåº¦æ€»æˆæœ¬: $2.40

print(f"èŠ‚çœ: ${total_cost - total_cost_b2:.2f}")
# è¾“å‡º: èŠ‚çœ: $18.10
```

---

## ğŸ”„ è¿ç§»ç›¸å…³

### Q18: å¦‚ä½•ä»æœ¬åœ°å­˜å‚¨è¿ç§»åˆ°S3ï¼Ÿ

**A**: æä¾›è‡ªåŠ¨è¿ç§»è„šæœ¬ã€‚

**migrate_to_s3.py**:
```python
#!/usr/bin/env python3
"""
æœ¬åœ°å­˜å‚¨ â†’ S3 è¿ç§»è„šæœ¬
"""

import os
import boto3
from pathlib import Path
from tqdm import tqdm

def migrate_to_s3(
    local_base: str,
    s3_bucket: str,
    client_ids: list
):
    """
    è¿ç§»æœ¬åœ°æ–‡ä»¶åˆ°S3
    
    Args:
        local_base: æœ¬åœ°åŸºç¡€ç›®å½•ï¼ˆä¾‹å¦‚: "."ï¼‰
        s3_bucket: S3æ¡¶å
        client_ids: è¦è¿ç§»çš„CLIENT_IDåˆ—è¡¨
    """
    s3_client = boto3.client('s3')
    
    for client_id in client_ids:
        print(f"\nğŸ“¦ å¼€å§‹è¿ç§» {client_id}...")
        
        # è¿ç§»å¹¿å‘Šè§†é¢‘
        ads_dir = Path(local_base) / "ads" / client_id
        if ads_dir.exists():
            files = list(ads_dir.glob("*.*"))
            
            for file_path in tqdm(files, desc=f"ä¸Šä¼  {client_id} å¹¿å‘Š"):
                s3_key = f"{client_id}/ads/{file_path.name}"
                
                with open(file_path, 'rb') as f:
                    s3_client.put_object(
                        Bucket=s3_bucket,
                        Key=s3_key,
                        Body=f.read(),
                        ContentType=get_content_type(file_path.name)
                    )
                
                print(f"âœ… {file_path.name} â†’ s3://{s3_bucket}/{s3_key}")
        
        # è¿ç§»Agentèµ„æº
        agent_dir = Path(local_base) / "agent" / client_id
        if agent_dir.exists():
            # ç±»ä¼¼å¤„ç†
            pass
    
    print("\nâœ… è¿ç§»å®Œæˆï¼")
    print(f"è¯·ä¿®æ”¹ conf.yaml ä¸­çš„ storage_type: s3")

def get_content_type(filename: str) -> str:
    ext = Path(filename).suffix.lower()
    content_types = {
        '.mp4': 'video/mp4',
        '.jpg': 'image/jpeg',
        '.png': 'image/png'
    }
    return content_types.get(ext, 'application/octet-stream')

if __name__ == "__main__":
    migrate_to_s3(
        local_base=".",
        s3_bucket="my-ads-bucket",
        client_ids=["client_001", "client_002", "client_003"]
    )
```

**ä½¿ç”¨æ­¥éª¤**:
```bash
# 1. é…ç½®AWSå‡­è¯
export AWS_ACCESS_KEY=xxx
export AWS_SECRET_KEY=xxx

# 2. è¿è¡Œè¿ç§»è„šæœ¬
python migrate_to_s3.py

# 3. ç­‰å¾…è¿ç§»å®Œæˆ
# ğŸ“¦ å¼€å§‹è¿ç§» client_001...
# âœ… video1.mp4 â†’ s3://my-bucket/client_001/ads/video1.mp4
# âœ… video2.mp4 â†’ s3://my-bucket/client_001/ads/video2.mp4
# âœ… è¿ç§»å®Œæˆï¼

# 4. ä¿®æ”¹é…ç½®
vim conf.yaml
# storage_type: "s3"

# 5. é‡å¯æœåŠ¡å™¨
python run_server.py

# âœ… ç°åœ¨ä½¿ç”¨S3å­˜å‚¨
```

---

### Q19: S3è¿ç§»åæœ¬åœ°æ–‡ä»¶å¯ä»¥åˆ é™¤å—ï¼Ÿ

**A**: å»ºè®®å…ˆ**å¤‡ä»½éªŒè¯**å†åˆ é™¤ã€‚

**å®‰å…¨è¿ç§»æµç¨‹**:
```bash
# 1. è¿ç§»åˆ°S3
python migrate_to_s3.py

# 2. éªŒè¯S3æ–‡ä»¶å®Œæ•´æ€§
aws s3 ls s3://my-bucket/client_001/ads/ --recursive

# 3. å¯¹æ¯”æœ¬åœ°å’ŒS3æ•°é‡
LOCAL_COUNT=$(find ads/client_001 -type f | wc -l)
S3_COUNT=$(aws s3 ls s3://my-bucket/client_001/ads/ | wc -l)

if [ "$LOCAL_COUNT" -eq "$S3_COUNT" ]; then
    echo "âœ… æ–‡ä»¶æ•°é‡ä¸€è‡´"
else
    echo "âŒ æ–‡ä»¶æ•°é‡ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥"
    exit 1
fi

# 4. æµ‹è¯•S3æ¨¡å¼èƒ½æ­£å¸¸å·¥ä½œ
# åˆ‡æ¢åˆ°storage_type: "s3"ï¼Œæµ‹è¯•1å‘¨

# 5. ç¡®è®¤æ— è¯¯åï¼Œå¤‡ä»½æœ¬åœ°æ–‡ä»¶
tar -czf ads_backup_$(date +%Y%m%d).tar.gz ads/

# 6. åˆ é™¤æœ¬åœ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
rm -rf ads/client_001/*

# ä¿ç•™ç›®å½•ç»“æ„ï¼Œæ–¹ä¾¿æœªæ¥å›æ»š
```

---

## ğŸ¯ æŠ€æœ¯é€‰å‹

### Q20: å¦‚ä½•é€‰æ‹©S3æœåŠ¡å•†ï¼Ÿ

**A**: æ ¹æ®éœ€æ±‚å’Œé¢„ç®—é€‰æ‹©ã€‚

**é€‰æ‹©çŸ©é˜µ**:

| éœ€æ±‚ | æ¨èæœåŠ¡å•† | åŸå›  |
|------|-----------|------|
| **å›½å†…ç”¨æˆ·** | é˜¿é‡Œäº‘OSS | âœ… é€Ÿåº¦å¿«<br/>âœ… ä¸­æ–‡æ”¯æŒ<br/>âœ… æœ¬åœ°åŒ– |
| **å›½é™…ç”¨æˆ·** | AWS S3 | âœ… å…¨çƒè¦†ç›–<br/>âœ… ç”Ÿæ€å®Œå–„<br/>âœ… ç¨³å®šå¯é  |
| **æˆæœ¬ä¼˜å…ˆ** | Backblaze B2 | âœ… ä»·æ ¼æœ€ä½<br/>âœ… Cloudflareå…è´¹CDN<br/>âœ… æ— éšè—è´¹ç”¨ |
| **è‡ªä¸»å¯æ§** | MinIO (è‡ªå»º) | âœ… å®Œå…¨æŒæ§<br/>âœ… æ— å¤–éƒ¨ä¾èµ–<br/>âŒ éœ€è¦è¿ç»´ |
| **æ”¿åºœ/é‡‘è** | ç§æœ‰äº‘S3 | âœ… åˆè§„è¦æ±‚<br/>âœ… æ•°æ®æœ¬åœ°åŒ–<br/>ğŸ’° æˆæœ¬é«˜ |

**æ€§ä»·æ¯”æ’è¡Œ**:
1. ğŸ¥‡ **Backblaze B2 + Cloudflare CDN** - æœ€çœé’±
2. ğŸ¥ˆ **é˜¿é‡Œäº‘OSS** - å›½å†…é€Ÿåº¦å¿«
3. ğŸ¥‰ **AWS S3 + CloudFront** - ç”Ÿæ€æœ€å®Œå–„
4. MinIOï¼ˆè‡ªå»ºï¼‰- å®Œå…¨å…è´¹ï¼Œéœ€è¿ç»´
5. è…¾è®¯äº‘COS - ä¸é˜¿é‡Œäº‘ç±»ä¼¼

---

### Q21: æ”¯æŒå¯¹è±¡å­˜å‚¨ä¹‹å¤–çš„äº‘å­˜å‚¨å—ï¼Ÿ

**A**: ç†è®ºä¸Šå¯ä»¥ï¼Œä½†éœ€è¦é¢å¤–å¼€å‘ã€‚

**å½“å‰æ”¯æŒï¼ˆS3åè®®ï¼‰**:
- âœ… AWS S3
- âœ… é˜¿é‡Œäº‘OSSï¼ˆS3å…¼å®¹æ¨¡å¼ï¼‰
- âœ… MinIO
- âœ… Backblaze B2

**ä¸ç›´æ¥æ”¯æŒï¼ˆéœ€è¦é€‚é…ï¼‰**:
- âŒ è…¾è®¯äº‘COSï¼ˆæœ‰S3å…¼å®¹æ¨¡å¼ï¼Œå¯ç”¨ï¼‰
- âŒ ä¸ƒç‰›äº‘Kodoï¼ˆéœ€è¦å¼€å‘KodoStorageServiceï¼‰
- âŒ åˆæ‹äº‘USSï¼ˆéœ€è¦å¼€å‘USSStorageServiceï¼‰

**æ‰©å±•æ–¹æ³•**:
```python
# æ–°å¢ KodoStorageService
class KodoStorageService(StorageInterface):
    """ä¸ƒç‰›äº‘Kodoå­˜å‚¨æœåŠ¡"""
    
    def __init__(self, client_id, bucket, access_key, secret_key):
        from qiniu import Auth, put_data
        self.qiniu_auth = Auth(access_key, secret_key)
        self.bucket = bucket
    
    async def upload_file(self, file_data, category, filename):
        # ä½¿ç”¨ä¸ƒç‰›SDKä¸Šä¼ 
        token = self.qiniu_auth.upload_token(self.bucket)
        ret, info = put_data(token, key, file_data)
        return key

# åœ¨storage_factoryä¸­æ·»åŠ 
if storage_type == "kodo":
    return KodoStorageService(...)
```

---

## ğŸ“Š ç›‘æ§å’Œç»´æŠ¤

### Q22: å¦‚ä½•ç›‘æ§S3ä½¿ç”¨æƒ…å†µï¼Ÿ

**A**: ä½¿ç”¨AWS CloudWatchæˆ–è‡ªå®šä¹‰ç›‘æ§ã€‚

**AWS CloudWatch**:
```bash
# æŸ¥çœ‹S3æ¡¶æŒ‡æ ‡
aws cloudwatch get-metric-statistics \
    --namespace AWS/S3 \
    --metric-name BucketSizeBytes \
    --dimensions Name=BucketName,Value=my-bucket \
    --start-time 2024-10-01T00:00:00Z \
    --end-time 2024-10-27T23:59:59Z \
    --period 86400 \
    --statistics Average

# æŸ¥çœ‹è¯·æ±‚æ•°
aws cloudwatch get-metric-statistics \
    --namespace AWS/S3 \
    --metric-name NumberOfObjects \
    ...
```

**è‡ªå®šä¹‰ç›‘æ§**:
```python
# å®šæœŸç»Ÿè®¡è„šæœ¬
import boto3

s3 = boto3.client('s3')

def get_bucket_stats(bucket: str):
    total_size = 0
    file_count = 0
    
    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=bucket):
        for obj in page.get('Contents', []):
            total_size += obj['Size']
            file_count += 1
    
    return {
        "file_count": file_count,
        "total_size_gb": total_size / (1024**3),
        "estimated_cost": total_size / (1024**3) * 0.023  # $0.023/GB
    }

stats = get_bucket_stats("my-bucket")
print(f"æ–‡ä»¶æ•°: {stats['file_count']}")
print(f"æ€»å¤§å°: {stats['total_size_gb']:.2f} GB")
print(f"ä¼°ç®—æˆæœ¬: ${stats['estimated_cost']:.2f}/æœˆ")
```

---

### Q23: å¦‚ä½•å¤‡ä»½S3æ•°æ®ï¼Ÿ

**A**: S3æœ¬èº«å·²ç»é«˜å¯ç”¨ï¼Œä½†ä»å»ºè®®è·¨åŒºåŸŸå¤åˆ¶ã€‚

**æ–¹æ¡ˆ1: S3è·¨åŒºåŸŸå¤åˆ¶ï¼ˆæ¨èï¼‰**
```bash
# å¯ç”¨è·¨åŒºåŸŸå¤åˆ¶
aws s3api put-bucket-replication \
    --bucket my-bucket \
    --replication-configuration file://replication.json
```

**replication.json**:
```json
{
    "Role": "arn:aws:iam::123456789:role/s3-replication-role",
    "Rules": [
        {
            "Status": "Enabled",
            "Priority": 1,
            "Filter": {},
            "Destination": {
                "Bucket": "arn:aws:s3:::my-bucket-backup",
                "ReplicationTime": {
                    "Status": "Enabled",
                    "Time": {"Minutes": 15}
                }
            }
        }
    ]
}
```

**æ–¹æ¡ˆ2: å®šæœŸåŒæ­¥åˆ°æœ¬åœ°**
```bash
# å®šæ—¶ä»»åŠ¡ï¼šæ¯å¤©å¤‡ä»½S3åˆ°æœ¬åœ°
#!/bin/bash
# backup_s3.sh

BACKUP_DIR="/backup/s3_$(date +%Y%m%d)"
mkdir -p "$BACKUP_DIR"

# åŒæ­¥S3åˆ°æœ¬åœ°
aws s3 sync s3://my-bucket "$BACKUP_DIR"

# å‹ç¼©å¤‡ä»½
tar -czf "$BACKUP_DIR.tar.gz" "$BACKUP_DIR"

# åˆ é™¤ä¸´æ—¶ç›®å½•
rm -rf "$BACKUP_DIR"

# ä¿ç•™æœ€è¿‘7å¤©çš„å¤‡ä»½
find /backup -name "s3_*.tar.gz" -mtime +7 -delete
```

**crontabé…ç½®**:
```cron
# æ¯å¤©å‡Œæ™¨3ç‚¹å¤‡ä»½
0 3 * * * /path/to/backup_s3.sh
```

---

## ğŸ¯ æ€»ç»“

### å…³é”®è¦ç‚¹

1. **é…ç½®ä¼˜å…ˆçº§**: ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶
2. **æˆæœ¬ä¼˜åŒ–**: ä½¿ç”¨B2+Cloudflareæœ€çœé’±
3. **å®‰å…¨ç¬¬ä¸€**: å‡­è¯ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œä¸å†™é…ç½®æ–‡ä»¶
4. **CDNå¿…å¤‡**: ç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨CDN
5. **CLIENT_IDéš”ç¦»**: ç¡®ä¿å¤šç§Ÿæˆ·æ•°æ®å®‰å…¨
6. **æ¸è¿›è¿ç§»**: å…ˆæµ‹è¯•å†åˆ‡æ¢ï¼Œä¿ç•™æœ¬åœ°å¤‡ä»½

### å¿«é€Ÿå‚è€ƒ

```bash
# åˆ‡æ¢åˆ°S3
storage_type: "s3"

# åˆ‡æ¢åˆ°æœ¬åœ°
storage_type: "local"

# æŸ¥çœ‹S3æ–‡ä»¶
aws s3 ls s3://my-bucket/client_001/ads/

# ä¸Šä¼ æµ‹è¯•
curl -F "file=@test.mp4" http://localhost:12393/api/upload?client=client_001

# å¥åº·æ£€æŸ¥
curl http://localhost:12393/api/health
```

---

**å®Œæ•´ç³»åˆ—ç›®å½•**:
- [01-é›†æˆæ€è·¯ä¸æ¶æ„è®¾è®¡](./01-é›†æˆæ€è·¯ä¸æ¶æ„è®¾è®¡.md)
- [02-å¹¿å‘Šè§†é¢‘S3ä¸Šä¼ è¯¦è§£](./02-å¹¿å‘Šè§†é¢‘S3ä¸Šä¼ è¯¦è§£.md)
- [03-Agentèµ„æºåŠ¨æ€ä¸Šä¼ ](./03-Agentèµ„æºåŠ¨æ€ä¸Šä¼ .md)
- [04-é¡¹ç›®æ•´ä½“æ¶æ„ä¸éƒ¨ç½²](./04-é¡¹ç›®æ•´ä½“æ¶æ„ä¸éƒ¨ç½².md)
- [05-ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—](./05-ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—.md)
- [06-å®æ–½è·¯çº¿å›¾](./06-å®æ–½è·¯çº¿å›¾.md)
- [07-å¸¸è§é—®é¢˜FAQ](./07-å¸¸è§é—®é¢˜FAQ.md) â† å½“å‰

