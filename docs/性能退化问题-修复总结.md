# 性能退化问题修复总结

## 📋 问题回顾

**使用场景**：单用户LED屏幕，通过浏览器访问
**问题症状**：
- 第1次打开：响应快速（~3秒）
- 重复打开/关闭几次后：响应变慢（6-9秒，2-3倍）
- 只有重启Docker才能恢复正常

## 🔍 根本原因

**MCP服务器进程累积泄漏**

每次前端连接时：
1. 创建新的 `ServiceContext`
2. 调用 `_init_mcp_components()`
3. 创建新的 `MCPClient`
4. 启动5个MCP服务器进程（你的配置）
5. Warm up这5个服务器连接

**问题**：旧的 `MCPClient` 在断开连接时没有被完全清理，导致：
```
第1次连接：5个MCP进程 → 关闭后残留
第2次连接：又启动5个 = 总共10个进程
第3次连接：又启动5个 = 总共15个进程  ← 开始变慢
第4次连接：又启动5个 = 总共20个进程  ← 非常慢（6-9秒）
```

每次对话需要等待所有进程响应，进程越多越慢。

---

## ✅ 修复内容

### 修改1：强化MCP Client关闭逻辑
**文件**：`src/ai_chat/mcpp/mcp_client.py`

```python
async def aclose(self):
    # ✅ 添加：显式关闭每个session（确保服务器进程被终止）
    for server_name, session in list(self.active_sessions.items()):
        try:
            await asyncio.wait_for(session.close(), timeout=2.0)
        except Exception as e:
            logger.warning(f"关闭session '{server_name}' 失败: {e}")
    
    # 原有逻辑：清理exit_stack
    await self.exit_stack.aclose()
    
    # 清空所有引用
    self.active_sessions.clear()
    self._list_tools_cache.clear()
```

**作用**：
- 显式关闭每个MCP session
- 确保服务器进程被正确终止
- 即使某个session关闭失败，继续清理其他

---

### 修改2：ServiceContext关闭时增加诊断日志
**文件**：`src/ai_chat/service_context.py`

```python
async def close(self):
    if self.mcp_client:
        # ✅ 添加：诊断日志
        logger.info(f"🔍 活跃MCP sessions: {len(self.mcp_client.active_sessions)}")
        logger.info(f"🔍 Sessions: {list(self.mcp_client.active_sessions.keys())}")
        
        # ✅ 添加：超时保护
        try:
            await asyncio.wait_for(self.mcp_client.aclose(), timeout=5.0)
            logger.info("✅ MCPClient已关闭")
        except asyncio.TimeoutError:
            logger.error("❌ MCPClient关闭超时！可能有服务器进程残留")
        except Exception as e:
            logger.error(f"❌ MCPClient关闭失败: {e}")
        finally:
            self.mcp_client = None
```

**作用**：
- 监控MCP Client的清理过程
- 及时发现清理失败的情况
- 超时保护，避免无限等待

---

### 修改3：防御性检查：初始化前清理旧Client
**文件**：`src/ai_chat/service_context.py`

```python
async def _init_mcp_components(self, use_mcpp, enabled_servers):
    # ✅ 添加：防御性检查
    if self.mcp_client:
        logger.warning("⚠️ 检测到旧MCP Client未清理，先关闭...")
        logger.info(f"🔍 旧Client活跃sessions: {len(self.mcp_client.active_sessions)}")
        try:
            await asyncio.wait_for(self.mcp_client.aclose(), timeout=3.0)
            logger.info("✅ 旧MCP Client已清理")
        except Exception as e:
            logger.error(f"❌ 清理旧MCP Client失败: {e}")
    
    # 然后再创建新的MCP Client
    # ...
```

**作用**：
- 防止在有旧Client残留时创建新Client
- 即使上层清理失败，这里也会兜底清理
- 多一层防护，确保不会累积

---

## 🎯 预期效果

修复后应该看到：

### ✅ 日志表现（正常）
```log
# 打开网页 - 第1次
🔧 初始化新的MCP组件 (client_uid: xxx)
MCPC: Initialized MCPClient instance.
... 启动5个MCP服务器 ...

# 关闭网页 - 第1次
🔌 开始清理客户端 xxx 的资源...
  🔍 活跃MCP sessions: 5
  🔍 Sessions: ['laundry-assistant', 'advertisement-server', 'time', 'weather-server', 'fukuoka-transit']
  🔄 关闭 session: laundry-assistant
  🔄 关闭 session: advertisement-server
  ...
  ✅ exit_stack已清理
  ✅ MCPClient已关闭

# 打开网页 - 第2次
🔧 初始化新的MCP组件 (client_uid: yyy)
⏭️ 无旧MCP Client（或已清理）
... 启动5个MCP服务器 ...

# 关闭网页 - 第2次
... 正常清理 ...
```

### ✅ 性能表现
```
第1次连接：3秒  ← 正常
第2次连接：3秒  ← 稳定
第3次连接：3秒  ← 稳定
第4次连接：3-4秒 ← 稳定
第10次连接：3-4秒 ← 仍然稳定
```

### ✅ 进程数量
```bash
# 连接时
ps aux | grep -E "(laundry|advertisement)" | wc -l
# 输出：5个进程 ✓

# 关闭后（等待5秒）
ps aux | grep -E "(laundry|advertisement)" | wc -l
# 输出：0个进程 ✓

# 第2次连接
ps aux | grep -E "(laundry|advertisement)" | wc -l
# 输出：5个进程 ✓（不是10个！）
```

---

## 🧪 如何验证修复

### 快速测试（5分钟）
1. 启动后端：`python run_server.py`
2. 打开浏览器访问
3. 说一句话，记录时间
4. 关闭浏览器
5. 重复步骤2-4，共5次
6. 对比每次的响应时间

**成功标志**：响应时间保持在3-4秒范围内

---

### 完整测试（15分钟）
按照 `docs/MCP连接泄漏修复与测试.md` 的详细步骤：
1. 观察初始状态
2. 重复连接/断开10次
3. 检查进程数量
4. 检查日志中的清理状态
5. 记录测试结果

---

## ❓ 如果修复后仍有问题

### 情况A：偶尔变慢

**可能原因**：某个MCP服务器不稳定
**解决方案**：
1. 检查日志，找到失败的服务器
2. 暂时禁用该服务器（在`conf.yaml`中移除）
3. 或增加该服务器的超时时间

---

### 情况B：仍然持续变慢

**可能原因**：
1. `handle_disconnect` 没有被正确调用
2. WebSocket连接没有正确断开
3. 其他资源泄漏（非MCP）

**诊断步骤**：
1. 检查日志是否有 `🔌 开始清理客户端` 的日志
2. 如果没有，说明 `handle_disconnect` 没有被调用
3. 需要检查WebSocket路由和连接管理

---

### 情况C：完全没有改善

**备选方案**：使用MCP组件复用（单用户优化）

见 `docs/MCP连接泄漏修复与测试.md` 的"方案1：复用MCP组件"

这会让所有连接共享同一套MCP组件，完全避免重复创建。

---

## 📊 技术细节

### 为什么需要显式关闭session？

原来的代码只调用了 `exit_stack.aclose()`，但这可能因为：
1. Context manager的清理顺序问题
2. 异步资源的清理延迟
3. 某些exception导致清理中断

导致服务器进程没有被正确终止。

通过显式调用每个 `session.close()`，我们确保：
1. 每个session都收到关闭信号
2. 服务器进程收到SIGTERM/SIGKILL
3. 即使某个失败，其他仍会清理

---

### 为什么添加超时保护？

防止某个MCP服务器卡住导致整个清理流程阻塞。

如果某个服务器在5秒内无法关闭：
- 记录错误日志（便于诊断）
- 继续清理其他资源
- 不阻塞后续连接

---

### 为什么需要防御性检查？

虽然理论上 `handle_disconnect` 会调用 `context.close()`，但可能因为：
1. 网络突然断开
2. 浏览器崩溃
3. 某个异步任务失败

导致 `close()` 没有被调用。

防御性检查确保：即使上层清理失败，下次创建新Client时也会先清理旧的。

---

## 🎉 总结

**核心修复**：
1. ✅ 显式关闭每个MCP session
2. ✅ 添加超时保护
3. ✅ 防御性清理检查
4. ✅ 详细的诊断日志

**预期效果**：
- 彻底解决MCP进程累积问题
- 支持无限次打开/关闭而不变慢
- 无需重启Docker即可长期运行

**下一步**：
请运行测试并告诉我结果！ 🚀

如果测试后仍有问题，我们还有备选方案（MCP组件复用）可以尝试。

