#!/usr/bin/env python3
"""
ä¸»é¢˜ä»‹ç»ç®¡ç† MCP æœåŠ¡å™¨
æä¾›ä¸»é¢˜ä»‹ç»æ•°æ®ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒå¤šè¯­è¨€å’Œå¤šç§Ÿæˆ·
"""

import json
import re
import sys
import asyncio
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Any
from loguru import logger

from mcp.server.models import InitializationOptions
from mcp.server import NotificationOptions, Server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    LoggingLevel
)
import mcp.types as types
from pydantic import AnyUrl


try:
    # Force UTF-8 output to avoid Windows GBK console crashes
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")
except Exception:
    pass


class SimpleMediaConfig:
    """ç®€åŒ–çš„åª’ä½“é…ç½®ç±»ï¼Œç”¨äºç‹¬ç«‹MCPæœåŠ¡å™¨"""
    def __init__(self):
        self.host = "127.0.0.1"
        self.port = 12393
        self.ads_directory = "ads"
        self.videos_directory = "videos"
    
    def get_directory_path(self, directory_type: str):
        """è·å–æŒ‡å®šç±»å‹ç›®å½•çš„è·¯å¾„"""
        if directory_type == 'ads':
            return Path(self.ads_directory)
        elif directory_type == 'videos':
            return Path(self.videos_directory)
        elif directory_type == 'topics':
            return Path("topics")
        else:
            raise ValueError(f"Unknown directory type: {directory_type}")
    
    def get_video_url(self, category: str, filename: str) -> str:
        """ç”Ÿæˆè§†é¢‘æ–‡ä»¶çš„URL"""
        return f"http://{self.host}:{self.port}/{category}/{filename}"


def get_media_config():
    """è·å–åª’ä½“æœåŠ¡å™¨é…ç½®"""
    try:
        # å°è¯•ä»ç³»ç»Ÿé…ç½®åŠ è½½
        from ..config_manager.utils import Config
        config = Config()
        return config.system_config.media_server
    except Exception as e:
        print(f"Warning: Failed to load full system config: {e}")
        
        # å°è¯•ç›´æ¥ä»YAMLåŠ è½½åª’ä½“æœåŠ¡å™¨é…ç½®
        try:
            import yaml
            from pathlib import Path
            
            config_path = Path("conf.yaml")
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    yaml_config = yaml.safe_load(f)
                    system_config = yaml_config.get('system_config', {})
                    media_server_config = system_config.get('media_server', {})
                    
                    # åˆ›å»ºç®€åŒ–é…ç½®å¯¹è±¡
                    config = SimpleMediaConfig()
                    config.host = media_server_config.get('host', '127.0.0.1')
                    config.port = media_server_config.get('port', 12393)
                    config.ads_directory = media_server_config.get('ads_directory', 'ads')
                    config.videos_directory = media_server_config.get('videos_directory', 'videos')
                    
                    print(f"Loaded media config from YAML: host={config.host}, port={config.port}")
                    return config
        except Exception as yaml_error:
            print(f"Warning: Failed to load YAML config: {yaml_error}")
        
        # æœ€åçš„fallback
        print("Using default media configuration")
        return SimpleMediaConfig()


def detect_language_simple(text: str) -> str:
    """ç®€å•çš„è¯­è¨€æ£€æµ‹ï¼ˆç”¨äºMCPæœåŠ¡å™¨ï¼‰"""
    try:
        from langdetect import detect
        return detect(text)
    except:
        # å¦‚æœlangdetectä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•è§„åˆ™
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return 'zh'
        elif any('\u3040' <= char <= '\u309f' or '\u30a0' <= char <= '\u30ff' for char in text):
            return 'ja'
        else:
            return 'en'


class TopicIntroductionServer:
    """ä¸»é¢˜ä»‹ç»ç®¡ç†æœåŠ¡å™¨"""
    
    def __init__(self, topics_dir: str = "topics", client_id: str = None):
        self.server = Server("topic-introduction-server")
        
        # è·å–åª’ä½“é…ç½®
        self.media_config = get_media_config()
        
        # è·å–CLIENT_ID
        import os
        self.client_id = client_id or os.getenv('CLIENT_ID') or getattr(self.media_config, 'client_id', 'default_client')
        
        try:
            base_topics_dir = self.media_config.get_directory_path('topics')
            # å¦‚æœæ˜¯å¤šç§Ÿæˆ·æ¨¡å¼ï¼Œæ·»åŠ CLIENT_IDå­ç›®å½•
            self.topics_dir = base_topics_dir / self.client_id
        except:
            # Fallback to provided directory
            self.topics_dir = Path(topics_dir) / self.client_id
        
        self.topics = {}
        self.supported_image_formats = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp'}
        self.supported_video_formats = {'.mp4', '.avi', '.mov', '.webm', '.mkv'}
        
        # ç¡®ä¿ä¸»é¢˜ç›®å½•å­˜åœ¨
        self.topics_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ‰€æœ‰ä¸»é¢˜
        self._load_topics()
        
        # æ³¨å†Œå·¥å…·å’Œèµ„æº
        self._register_tools()
        self._register_resources()
    
    def _load_topics(self):
        """åŠ è½½æ‰€æœ‰ä¸»é¢˜æ•°æ®"""
        self.topics.clear()
        
        if not self.topics_dir.exists():
            print(f"âš ï¸ Warning: Topics directory {self.topics_dir} does not exist for CLIENT {self.client_id}")
            self.topics_dir.mkdir(parents=True, exist_ok=True)
            return
        
        print(f"ğŸ“ æ‰«æä¸»é¢˜ç›®å½•: {self.topics_dir} (CLIENT_ID: {self.client_id})")
        
        # æ‰«ææ‰€æœ‰ä¸»é¢˜ç›®å½•
        for topic_dir in self.topics_dir.iterdir():
            if not topic_dir.is_dir():
                continue
            
            topic_json_path = topic_dir / "topic.json"
            if not topic_json_path.exists():
                continue
            
            try:
                with open(topic_json_path, 'r', encoding='utf-8') as f:
                    topic_data = json.load(f)
                
                topic_id = topic_data.get('topic_id', topic_dir.name)
                self.topics[topic_id] = topic_data
                topic_name = topic_data.get('name', 'Unknown')
                topic_lang = topic_data.get('language', 'ja')
                print(f"âœ… [{self.client_id}] åŠ è½½ä¸»é¢˜: {topic_name} (language: {topic_lang})")
                
            except Exception as e:
                print(f"âŒ Error loading topic {topic_dir.name}: {e}")
        
        print(f"\nğŸ“š ä¸»é¢˜æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ: {len(self.topics)} ä¸ªä¸»é¢˜å·²åŠ è½½")
    
    def _get_topic_by_name(self, topic_name: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®ä¸»é¢˜åç§°æŸ¥æ‰¾ä¸»é¢˜ï¼ˆæ”¯æŒå¤šè¯­è¨€æ¨¡ç³ŠåŒ¹é…ï¼‰"""
        if not topic_name:
            return None
        
        topic_name_lower = topic_name.lower().strip()
        candidates = []
        
        # æå–å…³é”®è¯ï¼ˆæ”¯æŒå¤šè¯­è¨€ï¼šä¸­æ–‡ã€æ—¥è¯­ã€è‹±è¯­ã€éŸ©è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ç­‰ï¼‰
        def extract_keywords(text):
            keywords = []
            
            # 1. æå–ä¸­æ–‡å­—ç¬¦ï¼ˆè¿ç»­çš„ä¸­æ–‡å­—ç¬¦ï¼‰
            chinese_chars = re.findall(r'[\u4e00-\u9fff]+', text)
            keywords.extend(chinese_chars)
            
            # 2. æå–æ—¥æ–‡å­—ç¬¦ï¼ˆå¹³å‡åå’Œç‰‡å‡åï¼‰
            japanese_chars = re.findall(r'[\u3040-\u309f\u30a0-\u30ff]+', text)
            keywords.extend(japanese_chars)
            
            # 3. æå–éŸ©è¯­å­—ç¬¦ï¼ˆéŸ©æ–‡å­—ç¬¦ï¼‰
            korean_chars = re.findall(r'[\uac00-\ud7a3]+', text)
            keywords.extend(korean_chars)
            
            # 4. æå–æ³°è¯­å­—ç¬¦
            thai_chars = re.findall(r'[\u0e00-\u0e7f]+', text)
            keywords.extend(thai_chars)
            
            # 5. æå–é˜¿æ‹‰ä¼¯è¯­å­—ç¬¦
            arabic_chars = re.findall(r'[\u0600-\u06ff]+', text)
            keywords.extend(arabic_chars)
            
            # 6. æå–ä¿„è¯­å­—ç¬¦ï¼ˆè¥¿é‡Œå°”å­—æ¯ï¼‰
            russian_chars = re.findall(r'[\u0400-\u04ff]+', text)
            keywords.extend(russian_chars)
            
            # 7. æå–æ‹‰ä¸è¯­ç³»å•è¯ï¼ˆè‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­ç­‰ï¼‰
            # åŒ¹é…å¸¦é‡éŸ³ç¬¦å·çš„æ‹‰ä¸å­—æ¯
            latin_words = re.findall(r'[a-zA-Z\u00c0-\u017f]{2,}', text)
            # è½¬æ¢ä¸ºå°å†™å¹¶è¿‡æ»¤å¸¸è§åœç”¨è¯ï¼ˆå¤šè¯­è¨€åœç”¨è¯ï¼‰
            stop_words = {
                # è‹±è¯­
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 
                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 
                'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
                # è¥¿ç­ç‰™è¯­
                'el', 'la', 'los', 'las', 'un', 'una', 'y', 'o', 'pero', 'en', 'de', 'con', 'por', 'para',
                'es', 'son', 'fue', 'fueron', 'ser', 'estar', 'tener', 'hacer',
                # æ³•è¯­
                'le', 'la', 'les', 'un', 'une', 'et', 'ou', 'mais', 'dans', 'de', 'avec', 'pour',
                'est', 'sont', 'Ã©tait', 'Ã©taient', 'Ãªtre', 'avoir', 'faire',
                # å¾·è¯­
                'der', 'die', 'das', 'ein', 'eine', 'und', 'oder', 'aber', 'in', 'von', 'mit', 'fÃ¼r',
                'ist', 'sind', 'war', 'waren', 'sein', 'haben', 'machen',
                # æ„å¤§åˆ©è¯­
                'il', 'la', 'lo', 'gli', 'le', 'un', 'una', 'e', 'o', 'ma', 'in', 'di', 'con', 'per',
                'Ã¨', 'sono', 'era', 'erano', 'essere', 'avere', 'fare',
                # é€šç”¨è¯æ±‡
                'hotel', 'theme', 'topic', 'the', 'topic', 'theme'
            }
            latin_words = [w.lower() for w in latin_words if w.lower() not in stop_words and len(w) >= 3]
            keywords.extend(latin_words)
            
            # 8. å¦‚æœå…³é”®è¯ä¸ºç©ºï¼Œå°è¯•æå–æ‰€æœ‰éASCIIå­—ç¬¦ä½œä¸ºå…³é”®è¯ï¼ˆå¤„ç†å…¶ä»–å°è¯­ç§ï¼‰
            if not keywords:
                # æå–æ‰€æœ‰éASCIIã€éæ ‡ç‚¹çš„è¿ç»­å­—ç¬¦
                non_ascii_words = re.findall(r'[^\x00-\x7f\s\-_\.ï¼Œã€‚ã€]+', text)
                if non_ascii_words:
                    keywords.extend(non_ascii_words)
                else:
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ï¼ˆå»é™¤æ ‡ç‚¹åï¼Œè½¬ä¸ºå°å†™ï¼‰
                    text_clean = re.sub(r'[\s\-_\.ï¼Œã€‚ã€]+', '', text).lower()
                    if text_clean:
                        keywords.append(text_clean)
            
            return keywords
        
        topic_keywords = extract_keywords(topic_name_lower)
        
        for topic in self.topics.values():
            name = topic.get('name', '')
            if not name:
                continue
            
            name_lower = name.lower().strip()
            
            # 1. ç²¾ç¡®åŒ¹é…ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            if name_lower == topic_name_lower:
                return topic
            
            # 2. åŒ…å«åŒ¹é…ï¼ˆtopic_nameåŒ…å«åœ¨nameä¸­ï¼Œæˆ–nameåŒ…å«åœ¨topic_nameä¸­ï¼‰
            if topic_name_lower in name_lower or name_lower in topic_name_lower:
                candidates.append((topic, 1))  # é«˜ä¼˜å…ˆçº§
                continue
            
            # 3. å…³é”®è¯åŒ¹é…ï¼ˆæå–çš„å…³é”®è¯æœ‰é‡å ï¼Œæ”¯æŒè·¨è¯­è¨€ï¼‰
            name_keywords = extract_keywords(name_lower)
            if topic_keywords and name_keywords:
                # æ£€æŸ¥å…³é”®è¯é‡å ï¼ˆç›´æ¥åŒ¹é…ï¼‰
                common_keywords = set(topic_keywords) & set(name_keywords)
                
                # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•è·¨è¯­è¨€åŒ¹é…ï¼ˆé€‚ç”¨äºä¸åŒè¯­è¨€ä½†æ„æ€ç›¸åŒçš„æƒ…å†µï¼‰
                if not common_keywords:
                    # è·¨è¯­è¨€åŒ¹é…ç­–ç•¥ï¼š
                    # 1. æ£€æŸ¥å…³é”®è¯æ•°é‡ç›¸ä¼¼æ€§ï¼ˆå¦‚æœå…³é”®è¯æ•°é‡ç›¸è¿‘ï¼Œå¯èƒ½æ˜¯ç¿»è¯‘å¯¹ç­‰ï¼‰
                    keyword_count_ratio = min(len(topic_keywords), len(name_keywords)) / max(len(topic_keywords), len(name_keywords))
                    
                    # 2. æ£€æŸ¥å…³é”®è¯å­—ç¬¦ä¸²çš„å­—ç¬¦é‡å åº¦ï¼ˆé€‚ç”¨äºåŒæºè¯æˆ–éŸ³è¯‘è¯ï¼‰
                    topic_keywords_str = ''.join(topic_keywords)
                    name_keywords_str = ''.join(name_keywords)
                    char_overlap = 0.0
                    if topic_keywords_str and name_keywords_str:
                        topic_chars = set(topic_keywords_str.lower())
                        name_chars = set(name_keywords_str.lower())
                        common_chars = topic_chars & name_chars
                        if topic_chars and name_chars:
                            char_overlap = len(common_chars) / max(len(topic_chars), len(name_chars))
                    
                    # 3. æ£€æŸ¥å…³é”®è¯é•¿åº¦ç›¸ä¼¼æ€§
                    if topic_keywords and name_keywords:
                        avg_topic_len = sum(len(k) for k in topic_keywords) / len(topic_keywords)
                        avg_name_len = sum(len(k) for k in name_keywords) / len(name_keywords)
                        len_ratio = min(avg_topic_len, avg_name_len) / max(avg_topic_len, avg_name_len) if max(avg_topic_len, avg_name_len) > 0 else 0
                    else:
                        len_ratio = 0
                    
                    # ç»¼åˆè¯„åˆ†ï¼šå¦‚æœå¤šä¸ªæŒ‡æ ‡éƒ½è¾ƒé«˜ï¼Œå¯èƒ½æ˜¯è·¨è¯­è¨€åŒ¹é…
                    cross_lang_score = (keyword_count_ratio * 0.3 + char_overlap * 0.4 + len_ratio * 0.3)
                    if cross_lang_score >= 0.35:  # ç»¼åˆè¯„åˆ†è¶…è¿‡35%ï¼Œè®¤ä¸ºæ˜¯è·¨è¯­è¨€åŒ¹é…
                        common_keywords = {'_cross_lang_match'}  # æ ‡è®°ä¸ºè·¨è¯­è¨€åŒ¹é…
                
                if common_keywords:
                    # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                    if '_cross_lang_match' in common_keywords:
                        # è·¨è¯­è¨€åŒ¹é…ï¼Œä¼˜å…ˆçº§è¾ƒä½ä½†å¯æ¥å—
                        keyword_similarity = 0.4
                        candidates.append((topic, 2.5 + (1 - keyword_similarity)))
                    else:
                        keyword_similarity = len(common_keywords) / max(len(topic_keywords), len(name_keywords))
                        if keyword_similarity >= 0.3:  # é™ä½é˜ˆå€¼åˆ°30%ï¼Œæé«˜åŒ¹é…æˆåŠŸç‡
                            candidates.append((topic, 2 + (1 - keyword_similarity)))  # åŒ¹é…åº¦è¶Šé«˜ï¼Œä¼˜å…ˆçº§æ•°å­—è¶Šå°
                    continue
            
            # 4. éƒ¨åˆ†åŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹åçš„åŒ¹é…ï¼‰
            name_clean = re.sub(r'[\s\-_\.]+', '', name_lower)
            topic_name_clean = re.sub(r'[\s\-_\.]+', '', topic_name_lower)
            if name_clean == topic_name_clean:
                candidates.append((topic, 3))  # ä¸­ä¼˜å…ˆçº§
                continue
            
            # 5. å­—ç¬¦ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆç®€å•çš„å­—ç¬¦é‡å æ£€æŸ¥ï¼‰
            name_chars = set(name_lower)
            topic_name_chars = set(topic_name_lower)
            if name_chars and topic_name_chars:
                common_chars = name_chars & topic_name_chars
                similarity = len(common_chars) / max(len(name_chars), len(topic_name_chars))
                # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¯å¯èƒ½çš„åŒ¹é…
                if similarity >= 0.5:
                    candidates.append((topic, 4 + (1 - similarity)))  # ç›¸ä¼¼åº¦è¶Šä½ï¼Œä¼˜å…ˆçº§æ•°å­—è¶Šå¤§
        
        # å¦‚æœæœ‰å€™é€‰ï¼Œè¿”å›ä¼˜å…ˆçº§æœ€é«˜çš„ï¼ˆæ•°å­—æœ€å°çš„ï¼‰
        if candidates:
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šä¼˜å…ˆçº§æ•°å­—è¶Šå°è¶Šå¥½
            candidates.sort(key=lambda x: x[1] if isinstance(x[1], (int, float)) else 999)
            best_match = candidates[0][0]
            matched_name = best_match.get('name', '')
            print(f"ğŸ” ä¸»é¢˜åç§°æ¨¡ç³ŠåŒ¹é…: '{topic_name}' -> '{matched_name}' (ä¼˜å…ˆçº§: {candidates[0][1]:.2f})")
            return best_match
        
        return None
    
    def _get_content(self, content: Any) -> str:
        """è·å–å†…å®¹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼‰"""
        # ç›´æ¥è¿”å›å­—ç¬¦ä¸²å†…å®¹ï¼Œä¸åšç¿»è¯‘
        # AIä¼šæ ¹æ®target_languageå‚æ•°è‡ªåŠ¨ç¿»è¯‘
        if isinstance(content, str):
            return content
        return str(content)
    
    def _register_tools(self):
        """æ³¨å†ŒMCPå·¥å…·"""
        
        @self.server.list_tools()
        async def handle_list_tools() -> list[types.Tool]:
            """è¿”å›å¯ç”¨çš„å·¥å…·åˆ—è¡¨"""
            return [
                types.Tool(
                    name="get_topic_list",
                    description="è·å–æ‰€æœ‰ä¸»é¢˜åˆ—è¡¨",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "target_language": {
                                "type": "string",
                                "description": "ç›®æ ‡è¯­è¨€ä»£ç  (ja/en/zhç­‰)ï¼Œå¯é€‰",
                                "default": None
                            }
                        },
                        "required": []
                    }
                ),
                types.Tool(
                    name="get_topic_info",
                    description="è·å–ä¸»é¢˜è¯¦ç»†ä¿¡æ¯ï¼ˆåç§°ã€æè¿°ç­‰ï¼‰ã€‚æ³¨æ„ï¼šå¦‚æœä¸»é¢˜æœ‰å›¾ç‰‡æˆ–è§†é¢‘ï¼Œè¯·ä½¿ç”¨get_topic_imageå’Œget_topic_videoå·¥å…·æ¥æ˜¾ç¤ºï¼Œä¸è¦ç›´æ¥è¯´å‡ºURLã€‚",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "topic_name": {
                                "type": "string",
                                "description": "ä¸»é¢˜åç§°ï¼ˆæ”¯æŒå¤šè¯­è¨€ï¼‰"
                            },
                            "target_language": {
                                "type": "string",
                                "description": "ç›®æ ‡è¯­è¨€ä»£ç  (ja/en/zhç­‰)ï¼Œå¯é€‰"
                            }
                        },
                        "required": ["topic_name"]
                    }
                ),
                types.Tool(
                    name="get_topic_video",
                    description="è·å–ä¸»é¢˜çš„æŒ‡å®šè§†é¢‘å¹¶åœ¨ç¬¬äºŒç”»å¸ƒä¸Šæ’­æ”¾ã€‚è¿”å›è§†é¢‘URLï¼ˆä»…ç”¨äºå‰ç«¯æ˜¾ç¤ºï¼ŒAIä¸åº”è¯´å‡ºURLï¼‰ã€‚AIåº”ä¿æŒé™éŸ³ï¼Œåªä»‹ç»è§†é¢‘å†…å®¹æè¿°ï¼Œç»å¯¹ä¸è¦è¯´å‡ºæˆ–æåŠä»»ä½•URLã€é“¾æ¥æˆ–åœ°å€ã€‚å½“ç”¨æˆ·è¯¢é—®ä¸»é¢˜çš„è§†é¢‘æ—¶ï¼Œåº”è‡ªåŠ¨è°ƒç”¨æ­¤å·¥å…·ã€‚ä¸»é¢˜åç§°æ”¯æŒå¤šè¯­è¨€æ¨¡ç³ŠåŒ¹é…ã€‚**é‡è¦ï¼šå¦‚æœç”¨æˆ·è¯·æ±‚'æ‰€æœ‰è§†é¢‘'æˆ–'ä¸»é¢˜çš„è§†é¢‘'ï¼ˆæœªæŒ‡å®šå…·ä½“ç´¢å¼•ï¼‰ï¼Œå¿…é¡»æŒ‰é¡ºåºè°ƒç”¨æ­¤å·¥å…·å¤šæ¬¡ï¼Œä»video_index=0å¼€å§‹ï¼Œæ¯æ¬¡é€’å¢1ï¼Œç›´åˆ°æ˜¾ç¤ºå®Œæ‰€æœ‰è§†é¢‘ã€‚æ¯æ¬¡è°ƒç”¨åç­‰å¾…è§†é¢‘æ’­æ”¾å®Œæˆå†è°ƒç”¨ä¸‹ä¸€ä¸ªã€‚**",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "topic_name": {
                                "type": "string",
                                "description": "ä¸»é¢˜åç§°ï¼ˆæ”¯æŒå¤šè¯­è¨€æ¨¡ç³ŠåŒ¹é…ï¼Œå¦‚ï¼š'å¤æ—¥å¿è€…é…’åº—'ã€'å¤æ—¥å¿è€…ãƒ›ãƒ†ãƒ«'ã€'Summer Ninja Hotel'ç­‰ï¼‰"
                            },
                            "video_index": {
                                "type": "integer",
                                "description": "è§†é¢‘ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼Œé»˜è®¤0ï¼‰ã€‚å¦‚æœç”¨æˆ·è¯·æ±‚æ‰€æœ‰è§†é¢‘ï¼Œå¿…é¡»ä»0å¼€å§‹æŒ‰é¡ºåºè°ƒç”¨ï¼Œæ¯æ¬¡é€’å¢1",
                                "default": 0
                            },
                            "target_language": {
                                "type": "string",
                                "description": "ç›®æ ‡è¯­è¨€ä»£ç  (ja/en/zhç­‰)ï¼Œå¯é€‰"
                            }
                        },
                        "required": ["topic_name"]
                    }
                ),
                types.Tool(
                    name="get_topic_image",
                    description="è·å–ä¸»é¢˜çš„æŒ‡å®šå›¾ç‰‡å¹¶æ˜¾ç¤ºåœ¨ç¬¬äºŒç”»å¸ƒä¸Šã€‚è¿”å›å›¾ç‰‡URLï¼ˆä»…ç”¨äºå‰ç«¯æ˜¾ç¤ºï¼ŒAIä¸åº”è¯´å‡ºURLï¼‰å’Œæè¿°ã€‚AIåº”è¯´è¯ä»‹ç»å›¾ç‰‡å†…å®¹æè¿°ï¼Œç»å¯¹ä¸è¦è¯´å‡ºæˆ–æåŠä»»ä½•URLã€é“¾æ¥æˆ–åœ°å€ã€‚å½“ç”¨æˆ·è¯¢é—®ä¸»é¢˜çš„å›¾ç‰‡æ—¶ï¼Œåº”è‡ªåŠ¨è°ƒç”¨æ­¤å·¥å…·ã€‚ä¸»é¢˜åç§°æ”¯æŒå¤šè¯­è¨€æ¨¡ç³ŠåŒ¹é…ã€‚**é‡è¦ï¼šå¦‚æœç”¨æˆ·è¯·æ±‚'æ‰€æœ‰å›¾ç‰‡'æˆ–'ä¸»é¢˜çš„å›¾ç‰‡'ï¼ˆæœªæŒ‡å®šå…·ä½“ç´¢å¼•ï¼‰ï¼Œå¿…é¡»æŒ‰é¡ºåºè°ƒç”¨æ­¤å·¥å…·å¤šæ¬¡ï¼Œä»image_index=0å¼€å§‹ï¼Œæ¯æ¬¡é€’å¢1ï¼Œç›´åˆ°æ˜¾ç¤ºå®Œæ‰€æœ‰å›¾ç‰‡ã€‚æ¯æ¬¡è°ƒç”¨åç­‰å¾…å›¾ç‰‡æ˜¾ç¤ºå®Œæˆå†è°ƒç”¨ä¸‹ä¸€ä¸ªã€‚**",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "topic_name": {
                                "type": "string",
                                "description": "ä¸»é¢˜åç§°ï¼ˆæ”¯æŒå¤šè¯­è¨€æ¨¡ç³ŠåŒ¹é…ï¼Œå¦‚ï¼š'å¤æ—¥å¿è€…é…’åº—'ã€'å¤æ—¥å¿è€…ãƒ›ãƒ†ãƒ«'ã€'Summer Ninja Hotel'ç­‰ï¼‰"
                            },
                            "image_index": {
                                "type": "integer",
                                "description": "å›¾ç‰‡ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼Œé»˜è®¤0ï¼‰ã€‚å¦‚æœç”¨æˆ·è¯·æ±‚æ‰€æœ‰å›¾ç‰‡ï¼Œå¿…é¡»ä»0å¼€å§‹æŒ‰é¡ºåºè°ƒç”¨ï¼Œæ¯æ¬¡é€’å¢1",
                                "default": 0
                            },
                            "target_language": {
                                "type": "string",
                                "description": "ç›®æ ‡è¯­è¨€ä»£ç  (ja/en/zhç­‰)ï¼Œå¯é€‰"
                            }
                        },
                        "required": ["topic_name"]
                    }
                ),
                types.Tool(
                    name="search_topics",
                    description="æ ¹æ®å…³é”®è¯æœç´¢ä¸»é¢˜",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "keyword": {
                                "type": "string",
                                "description": "æœç´¢å…³é”®è¯"
                            },
                            "target_language": {
                                "type": "string",
                                "description": "ç›®æ ‡è¯­è¨€ä»£ç  (ja/en/zhç­‰)ï¼Œå¯é€‰"
                            }
                        },
                        "required": ["keyword"]
                    }
                ),
                types.Tool(
                    name="refresh_topics",
                    description="é‡æ–°æ‰«æä¸»é¢˜ç›®å½•ï¼Œåˆ·æ–°ä¸»é¢˜åˆ—è¡¨ï¼ˆç”¨æˆ·ä¸Šä¼ æ–°ä¸»é¢˜åè‡ªåŠ¨è°ƒç”¨ï¼‰",
                    inputSchema={
                        "type": "object",
                        "properties": {},
                        "required": []
                    }
                ),
            ]
        
        @self.server.call_tool()
        async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent | types.ImageContent]:
            """å¤„ç†å·¥å…·è°ƒç”¨"""
            if name == "get_topic_list":
                return await self._get_topic_list(arguments)
            elif name == "get_topic_info":
                return await self._get_topic_info(arguments)
            elif name == "get_topic_video":
                return await self._get_topic_video(arguments)
            elif name == "get_topic_image":
                return await self._get_topic_image(arguments)
            elif name == "search_topics":
                return await self._search_topics(arguments)
            elif name == "refresh_topics":
                return await self._refresh_topics(arguments)
            else:
                return [types.TextContent(
                    type="text",
                    text=f"Unknown tool: {name}"
                )]
    
    def _register_resources(self):
        """æ³¨å†ŒMCPèµ„æº"""
        
        @self.server.list_resources()
        async def handle_list_resources() -> list[types.Resource]:
            """è¿”å›å¯ç”¨çš„èµ„æºåˆ—è¡¨"""
            resources = []
            
            for topic_id, topic_data in self.topics.items():
                topic_name = self._get_localized_content(topic_data.get('name', {}))
                resources.append(types.Resource(
                    uri=AnyUrl(f"topic://introduction/{topic_id}"),
                    name=f"Topic: {topic_name}",
                    description=f"ä¸»é¢˜ä»‹ç»: {topic_name}",
                    mimeType="application/json"
                ))
            
            return resources
    
    async def _get_topic_list(self, arguments: dict) -> list[types.TextContent]:
        """è·å–æ‰€æœ‰ä¸»é¢˜åˆ—è¡¨"""
        target_language = arguments.get("target_language")
        
        topic_list = []
        for topic_id, topic_data in self.topics.items():
            name = self._get_content(topic_data.get('name', ''))
            description = self._get_content(topic_data.get('description', ''))
            
            # æˆªæ–­é•¿æè¿°
            if len(description) > 100:
                description = description[:100] + "..."
            
            topic_info = {
                "topic_id": topic_id,
                "name": name,
                "description": description,
                "language": topic_data.get('language', 'ja'),
                "image_count": len(topic_data.get('images', [])),
                "video_count": len(topic_data.get('videos', [])),
                "target_language": target_language  # ä¼ é€’ç»™AIä½œä¸ºç¿»è¯‘æç¤º
            }
            topic_list.append(topic_info)
        
        return [types.TextContent(
            type="text",
            text=json.dumps({
                "type": "topic_list",
                "topics": topic_list,
                "total_count": len(topic_list),
                "target_language": target_language
            }, ensure_ascii=False)
        )]
    
    async def _get_topic_info(self, arguments: dict) -> list[types.TextContent]:
        """è·å–ä¸»é¢˜è¯¦ç»†ä¿¡æ¯"""
        topic_name = arguments.get("topic_name")
        target_language = arguments.get("target_language")
        
        topic = self._get_topic_by_name(topic_name)
        if not topic:
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": f"æœªæ‰¾åˆ°ä¸»é¢˜: {topic_name}"
                }, ensure_ascii=False)
            )]
        
        # æ„å»ºå“åº”æ•°æ®ï¼ˆä¿ç•™åŸå§‹è¯­è¨€ï¼ŒAIä¼šè‡ªåŠ¨ç¿»è¯‘ï¼‰
        response_data = {
            "type": "topic_info",
            "topic_id": topic.get('topic_id'),
            "name": self._get_content(topic.get('name', '')),
            "description": self._get_content(topic.get('description', '')),
            "language": topic.get('language', 'ja'),
            "target_language": target_language,  # ä¼ é€’ç»™AIä½œä¸ºç¿»è¯‘æç¤º
            "images": [],
            "videos": []
        }
        
        # å¤„ç†å›¾ç‰‡ï¼ˆåªè¿”å›æ•°é‡å’Œæç¤ºï¼Œä¸è¿”å›URLï¼‰
        image_count = len(topic.get('images', []))
        if image_count > 0:
            response_data["images"] = {
                "count": image_count,
                "hint": f"è¯¥ä¸»é¢˜æœ‰{image_count}å¼ å›¾ç‰‡ã€‚å¦‚æœç”¨æˆ·è¯·æ±‚æ‰€æœ‰å›¾ç‰‡ï¼Œå¿…é¡»æŒ‰é¡ºåºè°ƒç”¨get_topic_imageå·¥å…·{image_count}æ¬¡ï¼Œä»image_index=0å¼€å§‹ï¼Œæ¯æ¬¡é€’å¢1ï¼Œç›´åˆ°æ˜¾ç¤ºå®Œæ‰€æœ‰å›¾ç‰‡ã€‚æ¯æ¬¡è°ƒç”¨åç­‰å¾…å›¾ç‰‡æ˜¾ç¤ºå®Œæˆå†è°ƒç”¨ä¸‹ä¸€ä¸ªã€‚"
            }
        
        # å¤„ç†è§†é¢‘ï¼ˆåªè¿”å›æ•°é‡å’Œæç¤ºï¼Œä¸è¿”å›URLï¼‰
        video_count = len(topic.get('videos', []))
        if video_count > 0:
            response_data["videos"] = {
                "count": video_count,
                "hint": f"è¯¥ä¸»é¢˜æœ‰{video_count}ä¸ªè§†é¢‘ã€‚å¦‚æœç”¨æˆ·è¯·æ±‚æ‰€æœ‰è§†é¢‘ï¼Œå¿…é¡»æŒ‰é¡ºåºè°ƒç”¨get_topic_videoå·¥å…·{video_count}æ¬¡ï¼Œä»video_index=0å¼€å§‹ï¼Œæ¯æ¬¡é€’å¢1ï¼Œç›´åˆ°æ’­æ”¾å®Œæ‰€æœ‰è§†é¢‘ã€‚æ¯æ¬¡è°ƒç”¨åç­‰å¾…è§†é¢‘æ’­æ”¾å®Œæˆå†è°ƒç”¨ä¸‹ä¸€ä¸ªã€‚"
            }
        
        return [types.TextContent(
            type="text",
            text=json.dumps(response_data, ensure_ascii=False)
        )]
    
    async def _get_topic_video(self, arguments: dict) -> list[types.TextContent]:
        """è·å–ä¸»é¢˜çš„æŒ‡å®šè§†é¢‘"""
        topic_name = arguments.get("topic_name")
        video_index = arguments.get("video_index", 0)
        target_language = arguments.get("target_language")
        
        topic = self._get_topic_by_name(topic_name)
        if not topic:
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": f"æœªæ‰¾åˆ°ä¸»é¢˜: {topic_name}"
                }, ensure_ascii=False)
            )]
        
        videos = topic.get('videos', [])
        if not videos:
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": f"ä¸»é¢˜ '{topic_name}' æ²¡æœ‰è§†é¢‘"
                }, ensure_ascii=False)
            )]
        
        if video_index >= len(videos):
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": f"è§†é¢‘ç´¢å¼• {video_index} è¶…å‡ºèŒƒå›´ï¼ˆå…± {len(videos)} ä¸ªè§†é¢‘ï¼‰"
                }, ensure_ascii=False)
            )]
        
        video = videos[video_index]
        # ç”Ÿæˆå®Œæ•´çš„è§†é¢‘URL
        url_path = video.get('url_path', '')
        if url_path.startswith('/'):
            url_path = url_path[1:]
        video_url = f"http://{self.media_config.host}:{self.media_config.port}/{url_path}"
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šè§†é¢‘
        total_videos = len(videos)
        has_more = video_index < total_videos - 1
        next_index = video_index + 1 if has_more else None
        
        # è¿”å›è§†é¢‘ä¿¡æ¯ï¼Œæ ¼å¼åŒ–ä¸ºå‰ç«¯å¯ç”¨çš„æ ¼å¼
        return [types.TextContent(
            type="text",
            text=json.dumps({
                "type": "video",
                "url": video_url,
                "description": self._get_content(video.get('description', '')),
                "filename": video.get('filename'),
                "topic_name": self._get_content(topic.get('name', '')),
                "language": topic.get('language', 'ja'),
                "target_language": target_language,  # ä¼ é€’ç»™AIä½œä¸ºç¿»è¯‘æç¤º
                "video_index": video_index,
                "total_videos": total_videos,
                "has_more": has_more,
                "next_index": next_index,
                "hint": f"è¿™æ˜¯ç¬¬{video_index + 1}ä¸ªè§†é¢‘ï¼ˆå…±{total_videos}ä¸ªï¼‰ã€‚{'è¿˜æœ‰æ›´å¤šè§†é¢‘ï¼Œè¯·ç»§ç»­è°ƒç”¨get_topic_videoå·¥å…·ï¼Œvideo_index=' + str(next_index) if has_more else 'æ‰€æœ‰è§†é¢‘å·²æ˜¾ç¤ºå®Œæ¯•ã€‚'}"
            }, ensure_ascii=False)
        )]
    
    async def _get_topic_image(self, arguments: dict) -> list[types.TextContent | types.ImageContent]:
        """è·å–ä¸»é¢˜çš„æŒ‡å®šå›¾ç‰‡"""
        topic_name = arguments.get("topic_name")
        image_index = arguments.get("image_index", 0)
        target_language = arguments.get("target_language")
        
        topic = self._get_topic_by_name(topic_name)
        if not topic:
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": f"æœªæ‰¾åˆ°ä¸»é¢˜: {topic_name}"
                }, ensure_ascii=False)
            )]
        
        images = topic.get('images', [])
        if not images:
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": f"ä¸»é¢˜ '{topic_name}' æ²¡æœ‰å›¾ç‰‡"
                }, ensure_ascii=False)
            )]
        
        if image_index >= len(images):
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": f"å›¾ç‰‡ç´¢å¼• {image_index} è¶…å‡ºèŒƒå›´ï¼ˆå…± {len(images)} å¼ å›¾ç‰‡ï¼‰"
                }, ensure_ascii=False)
            )]
        
        image = images[image_index]
        # ç”Ÿæˆå®Œæ•´çš„å›¾ç‰‡URL
        url_path = image.get('url_path', '')
        if url_path.startswith('/'):
            url_path = url_path[1:]
        image_url = f"http://{self.media_config.host}:{self.media_config.port}/{url_path}"
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šå›¾ç‰‡
        total_images = len(images)
        has_more = image_index < total_images - 1
        next_index = image_index + 1 if has_more else None
        
        # è¿”å›å›¾ç‰‡ä¿¡æ¯ï¼Œæ ¼å¼åŒ–ä¸ºå‰ç«¯å¯ç”¨çš„æ ¼å¼
        return [types.TextContent(
            type="text",
            text=json.dumps({
                "type": "image",
                "url": image_url,
                "description": self._get_content(image.get('description', '')),
                "filename": image.get('filename'),
                "topic_name": self._get_content(topic.get('name', '')),
                "language": topic.get('language', 'ja'),
                "target_language": target_language,  # ä¼ é€’ç»™AIä½œä¸ºç¿»è¯‘æç¤º
                "image_index": image_index,
                "total_images": total_images,
                "has_more": has_more,
                "next_index": next_index,
                "hint": f"è¿™æ˜¯ç¬¬{image_index + 1}å¼ å›¾ç‰‡ï¼ˆå…±{total_images}å¼ ï¼‰ã€‚{'è¿˜æœ‰æ›´å¤šå›¾ç‰‡ï¼Œè¯·ç»§ç»­è°ƒç”¨get_topic_imageå·¥å…·ï¼Œimage_index=' + str(next_index) if has_more else 'æ‰€æœ‰å›¾ç‰‡å·²æ˜¾ç¤ºå®Œæ¯•ã€‚'}"
            }, ensure_ascii=False)
        )]
    
    async def _search_topics(self, arguments: dict) -> list[types.TextContent]:
        """æ ¹æ®å…³é”®è¯æœç´¢ä¸»é¢˜"""
        keyword = arguments.get("keyword", "").lower()
        target_language = arguments.get("target_language")
        
        if not keyword:
            return [types.TextContent(
                type="text",
                text=json.dumps({
                    "type": "error",
                    "message": "æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º"
                }, ensure_ascii=False)
            )]
        
        matched_topics = []
        for topic_id, topic_data in self.topics.items():
            # æœç´¢ä¸»é¢˜åç§°ï¼ˆç®€å•å­—ç¬¦ä¸²ï¼‰
            name = self._get_content(topic_data.get('name', ''))
            description = self._get_content(topic_data.get('description', ''))
            
            if keyword in name.lower() or keyword in description.lower():
                # æˆªæ–­é•¿æè¿°
                desc_preview = description[:100] + "..." if len(description) > 100 else description
                
                matched_topics.append({
                    "topic_id": topic_id,
                    "name": name,
                    "description": desc_preview,
                    "language": topic_data.get('language', 'ja')
                })
        
        return [types.TextContent(
            type="text",
            text=json.dumps({
                "type": "search_results",
                "keyword": keyword,
                "topics": matched_topics,
                "total_count": len(matched_topics),
                "target_language": target_language  # ä¼ é€’ç»™AIä½œä¸ºç¿»è¯‘æç¤º
            }, ensure_ascii=False)
        )]
    
    async def _refresh_topics(self, arguments: dict) -> list[types.TextContent]:
        """åˆ·æ–°ä¸»é¢˜åˆ—è¡¨ï¼ˆé‡æ–°æ‰«æç›®å½•ï¼‰"""
        old_count = len(self.topics)
        self._load_topics()
        new_count = len(self.topics)
        
        return [types.TextContent(
            type="text",
            text=json.dumps({
                "type": "refresh_response",
                "old_count": old_count,
                "new_count": new_count,
                "message": f"ä¸»é¢˜åˆ—è¡¨å·²åˆ·æ–°: {old_count} â†’ {new_count}",
                "topics": list(self.topics.values())
            }, ensure_ascii=False)
        )]


    async def run(self):
        """è¿è¡ŒæœåŠ¡å™¨"""
        from mcp.server.stdio import stdio_server
        
        try:
            async with stdio_server() as (read_stream, write_stream):
                await self.server.run(
                    read_stream,
                    write_stream,
                    InitializationOptions(
                        server_name="topic-introduction-server",
                        server_version="1.0.0",
                        capabilities=self.server.get_capabilities(
                            notification_options=NotificationOptions(),
                            experimental_capabilities={}
                        )
                    )
                )
        except (asyncio.CancelledError, KeyboardInterrupt) as e:
            print(f"ğŸ›‘ Topic introduction server stopped: {type(e).__name__}")
        except Exception as e:
            print(f"âŒ Topic introduction server error: {e}")
            import traceback
            traceback.print_exc()


async def main():
    """ä¸»å‡½æ•° - å¸¦æœ‰è‡ªåŠ¨é‡å¯æœºåˆ¶"""
    parser = argparse.ArgumentParser(description="ä¸»é¢˜ä»‹ç»MCPæœåŠ¡å™¨")
    parser.add_argument("--topics-dir", type=str, default="topics", help="ä¸»é¢˜ç›®å½•è·¯å¾„")
    parser.add_argument("--client-id", type=str, default=None, help="å®¢æˆ·ID")
    args = parser.parse_args()
    
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            server = TopicIntroductionServer(topics_dir=args.topics_dir, client_id=args.client_id)
            print(f"ğŸš€ ä¸»é¢˜ä»‹ç»MCPæœåŠ¡å™¨å·²å¯åŠ¨ (CLIENT_ID: {server.client_id})")
            await server.run()
            break  # æ­£å¸¸é€€å‡ºï¼Œä¸é‡è¯•
        except (asyncio.CancelledError, KeyboardInterrupt):
            print("ğŸ›‘ æœåŠ¡å™¨è¢«ç”¨æˆ·ä¸­æ–­")
            break  # ç”¨æˆ·ä¸­æ–­ï¼Œä¸é‡è¯•
        except Exception as e:
            retry_count += 1
            print(f"âŒ æœåŠ¡å™¨å´©æºƒ (å°è¯• {retry_count}/{max_retries}): {e}")
            if retry_count < max_retries:
                print(f"â³ ç­‰å¾… 3 ç§’åé‡å¯...")
                await asyncio.sleep(3)
            else:
                print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæœåŠ¡å™¨åœæ­¢")
                raise


if __name__ == "__main__":
    asyncio.run(main())

